# 4. Query Operator Status

This guide covers monitoring and querying your Fiamma Operator status in the **mainnet production** environment.

## Overview

Monitoring operator status is critical for production operations to:
- **Ensure continuous operation** and service availability
- **Track performance metrics** and rewards
- **Detect issues early** before they impact operations
- **Maintain compliance** with network requirements

## Basic Status Commands

### Service Status

Check the fundamental service health:

```bash
# Check systemd service status
sudo systemctl status fiamma-operator

# For GPG-encrypted deployments
systemctl --user status fiamma-operator

# Expected output indicators:
# â— fiamma-operator.service - Fiamma Operator
#    Loaded: loaded (/etc/systemd/system/fiamma-operator.service; enabled)
#    Active: active (running) since [timestamp]
#    Main PID: [process_id] (fiamma-operator)
```

### Real-time Logs

Monitor live operator activity:

```bash
# View real-time logs
tail -f .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log

# Filter for specific events
tail -f .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | grep -E "(ERROR|WARN|stake|reward|bridge)"

# Monitor system service logs
sudo journalctl -u fiamma-operator -f

# For user service (GPG encrypted)
journalctl --user -u fiamma-operator -f
```

## Operator Status Indicators

### Core Metrics

```bash
# Check these key indicators in logs:

# 1. Service Health
grep -i "health\|status" .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | tail -5

# 2. Network Connectivity
grep -i "connect\|network\|peer" .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | tail -5

# 3. Stake Status
grep -i "stake\|deposit" .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | tail -5

# 4. Bridge Operations
grep -i "bridge\|transaction\|operation" .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | tail -5
```

### Status Categories

```
ðŸŸ¢ Operational - All systems functioning normally
   â”œâ”€â”€ Service: Running
   â”œâ”€â”€ Network: Connected
   â”œâ”€â”€ Stake: Active
   â””â”€â”€ Operations: Processing

ðŸŸ¡ Warning - Minor issues, operation continues
   â”œâ”€â”€ Service: Running with warnings
   â”œâ”€â”€ Network: Intermittent connectivity
   â”œâ”€â”€ Stake: Partial operations
   â””â”€â”€ Operations: Delayed processing

ðŸ”´ Critical - Immediate attention required
   â”œâ”€â”€ Service: Stopped or failing
   â”œâ”€â”€ Network: Disconnected
   â”œâ”€â”€ Stake: At risk or slashed
   â””â”€â”€ Operations: Failed or halted

âšª Unknown - Status cannot be determined
   â”œâ”€â”€ Service: Unknown state
   â”œâ”€â”€ Network: Cannot determine
   â”œâ”€â”€ Stake: Status unclear
   â””â”€â”€ Operations: No recent activity
```

## Detailed Monitoring

### Performance Metrics

```bash
# System resource usage
echo "=== System Resources ==="
top -p $(pgrep fiamma-operator) -n 1 | tail -n +8

# Memory usage
echo "=== Memory Usage ==="
ps aux | grep fiamma-operator | grep -v grep

# Disk usage
echo "=== Disk Usage ==="
df -h | grep -E "(Filesystem|/dev/)"
du -sh .logs/ | head -5

# Network connections
echo "=== Network Connections ==="
sudo netstat -tulpn | grep fiamma-operator
```

### Database Status

```bash
# Check database connectivity
echo "=== Database Status ==="
sudo docker ps | grep postgres

# Database connection test
cd dal
sqlx database ping --database-url $(grep DATABASE_URL .env | cut -d '=' -f2) 2>/dev/null && echo "Database: OK" || echo "Database: FAILED"
cd ..

# Check Redis status
sudo docker ps | grep redis
```

### Bitcoin Network Status

```bash
# Bitcoin network connectivity indicators
echo "=== Bitcoin Network ==="
grep -i "bitcoin.*block\|bitcoin.*peer\|bitcoin.*sync" .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | tail -5

# Latest block information
grep -i "latest.*block\|best.*block\|current.*height" .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | tail -3
```

## Advanced Status Queries

### API Status (if available)

```bash
# If operator exposes status API endpoints:
# Check service health
curl -s http://localhost:8080/health | jq '.' 2>/dev/null || curl -s http://localhost:8080/health

# Check operator status
curl -s http://localhost:8080/api/status | jq '.' 2>/dev/null || curl -s http://localhost:8080/api/status

# Check stake information
curl -s http://localhost:8080/api/stake | jq '.' 2>/dev/null || curl -s http://localhost:8080/api/stake
```

### Log Analysis Scripts

Create monitoring scripts for regular status checks:

```bash
# Create status monitoring script
cat > check_operator_status.sh << 'EOF'
#!/bin/bash

echo "=== Fiamma Operator Status Report ==="
echo "Timestamp: $(date)"
echo

# Service status
echo "=== Service Status ==="
systemctl is-active fiamma-operator 2>/dev/null || systemctl --user is-active fiamma-operator 2>/dev/null
echo

# Last 10 log entries
echo "=== Recent Activity ==="
tail -10 .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log
echo

# Error count in last hour
echo "=== Error Count (Last Hour) ==="
since_time=$(date -d '1 hour ago' +%Y-%m-%d\ %H:%M)
grep -c "ERROR" .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log 2>/dev/null || echo "0"
echo

# System resources
echo "=== Resource Usage ==="
ps aux | grep fiamma-operator | grep -v grep | awk '{print "CPU: " $3 "%, Memory: " $4 "%"}'
echo

echo "=== Report Complete ==="
EOF

chmod +x check_operator_status.sh
```

## Production Monitoring Setup

### Automated Monitoring

```bash
# Set up cron job for regular status checks
crontab -e

# Add these lines for automated monitoring:
# Check status every 5 minutes
*/5 * * * * cd /path/to/operator && ./check_operator_status.sh >> /var/log/operator_monitoring.log 2>&1

# Daily comprehensive report
0 9 * * * cd /path/to/operator && ./daily_operator_report.sh | mail -s "Daily Operator Report" admin@yourdomain.com
```

### Alerting Setup

```bash
# Create alerting script
cat > alert_operator_issues.sh << 'EOF'
#!/bin/bash

LOG_FILE=".logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log"
ALERT_FILE="/tmp/operator_alerts_$(date +%Y%m%d).flag"

# Check for critical errors in last 10 minutes
ERRORS=$(grep -c "ERROR\|CRITICAL\|FATAL" "$LOG_FILE" 2>/dev/null || echo "0")

if [ "$ERRORS" -gt 0 ] && [ ! -f "$ALERT_FILE" ]; then
    echo "ALERT: $ERRORS errors detected in operator logs" | mail -s "Operator Alert" admin@yourdomain.com
    touch "$ALERT_FILE"
fi

# Check service status
if ! systemctl is-active --quiet fiamma-operator && ! systemctl --user is-active --quiet fiamma-operator; then
    echo "CRITICAL: Fiamma Operator service is not running" | mail -s "CRITICAL: Operator Down" admin@yourdomain.com
fi
EOF

chmod +x alert_operator_issues.sh
```

### Dashboard Creation

```bash
# Create simple HTML dashboard
cat > generate_dashboard.sh << 'EOF'
#!/bin/bash

HTML_FILE="/var/www/html/operator-status.html"
mkdir -p $(dirname "$HTML_FILE")

cat > "$HTML_FILE" << 'HTMLEOF'
<!DOCTYPE html>
<html>
<head>
    <title>Fiamma Operator Status</title>
    <meta http-equiv="refresh" content="30">
</head>
<body>
    <h1>Fiamma Operator Status Dashboard</h1>
    <p>Last Updated: DATE_PLACEHOLDER</p>
    
    <h2>Service Status</h2>
    <pre>SERVICE_STATUS_PLACEHOLDER</pre>
    
    <h2>Recent Activity</h2>
    <pre>RECENT_ACTIVITY_PLACEHOLDER</pre>
    
    <h2>System Resources</h2>
    <pre>SYSTEM_RESOURCES_PLACEHOLDER</pre>
</body>
</html>
HTMLEOF

# Replace placeholders with actual data
sed -i "s/DATE_PLACEHOLDER/$(date)/" "$HTML_FILE"
sed -i "s/SERVICE_STATUS_PLACEHOLDER/$(systemctl is-active fiamma-operator 2>/dev/null || systemctl --user is-active fiamma-operator 2>/dev/null)/" "$HTML_FILE"
sed -i "s/RECENT_ACTIVITY_PLACEHOLDER/$(tail -5 .logs/bitvm-operator/bitvm-operator.$(date +%Y-%m-%d).log | html_escape)/" "$HTML_FILE"
sed -i "s/SYSTEM_RESOURCES_PLACEHOLDER/$(ps aux | grep fiamma-operator | grep -v grep)/" "$HTML_FILE"
EOF

chmod +x generate_dashboard.sh
```

## Troubleshooting Status Issues

### Service Not Responding

```bash
# If status commands hang or fail:

# 1. Check if process exists
pgrep fiamma-operator

# 2. Check system resources
free -m
df -h

# 3. Check for zombie processes
ps aux | grep fiamma-operator | grep -E "(defunct|<defunct>)"

# 4. Restart if necessary
sudo systemctl restart fiamma-operator
# or for user service:
systemctl --user restart fiamma-operator
```

### Log File Issues

```bash
# If logs are not updating:

# Check log file permissions
ls -la .logs/bitvm-operator/

# Check disk space
df -h .

# Check log rotation
sudo logrotate -d /etc/logrotate.d/fiamma-operator

# Manual log rotation if needed
gzip .logs/bitvm-operator/bitvm-operator.$(date -d yesterday +%Y-%m-%d).log
```

### Network Connectivity Issues

```bash
# Test Bitcoin network connectivity
nc -z mainnet.bitcoin.org 8333

# Test DNS resolution
nslookup bitcoin.org

# Check firewall rules
sudo ufw status

# Test internet connectivity
ping -c 3 8.8.8.8
```

## Performance Optimization

### Resource Monitoring

```bash
# Monitor resource usage trends
sar -u 1 10  # CPU usage
sar -r 1 10  # Memory usage
sar -d 1 10  # Disk I/O

# Check for resource bottlenecks
iostat -x 1 5
vmstat 1 5
```

### Log Management

```bash
# Set up log rotation to prevent disk space issues
sudo tee /etc/logrotate.d/fiamma-operator << 'EOF'
/path/to/operator/.logs/bitvm-operator/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 644 $(whoami) $(whoami)
}
EOF
```

## Next Steps

For ongoing operator management:

1. **Proceed to [Manage the Operator Program](5.-manage-the-operator-program.md)** for maintenance procedures
2. **Implement automated monitoring** based on the scripts above
3. **Set up alerting** for critical issues
4. **Establish maintenance schedules** for optimal performance

## Support

For status monitoring assistance:
- **Monitoring Setup**: Technical support team
- **Performance Issues**: Performance optimization team
- **Critical Alerts**: Priority support channels
- **Dashboard Questions**: Integration support

---

**Next**: After establishing monitoring, proceed to [Manage the Operator Program](5.-manage-the-operator-program.md) for ongoing maintenance and management procedures.
